{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"webgraph2.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"GG3we0hxRiB9"},"source":["!unzip enron.zip -d data\n","#First upload enron.zip, then unzip it and restore it in data folder\n","#data folder path is /content/data/"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"UqRP6z2cF0pU"},"source":["!git clone https://github.com/kritishrivastava/AnomalyDetection.git"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gTxeRe6iU6Fo","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1636058320627,"user_tz":240,"elapsed":1133,"user":{"displayName":"Maloy Kumar Devnath","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08683219606020583205"}},"outputId":"16014da2-99f6-4598-caab-7e2468cbdcf6"},"source":["import glob\n","import re\n","import hashlib\n","import networkx as nx\n","import time\n","import numpy\n","import operator\n","import math\n","import sys\n","\n","#This function was copied from http://stackoverflow.com/questions/2545532/python-analog-of-natsort-function-sort-a-list-using-a-natural-order-algorithm\n","#It allows for natural sorting for the input files, so they're read in the correct order\n","def natural_key(string_):\n","    return [int(s) if s.isdigit() else s for s in re.split(r'(\\d+)', string_)]\n","\n","#Get command line args\n","graphFilePath = sys.argv[1:]\n","graphFilePath = str(\"/content/data\")\n","#graphFilePath = \"\"\n","print (\"Loading files from folder: \" + graphFilePath)\n","\n","start_time = time.clock()\n","\n","#Load dataset\n","# files = [\"datasets/enron/0_enron_by_day.txt\", \"datasets/enron/1_enron_by_day.txt\", \"datasets/enron/2_enron_by_day.txt\",\n","# \"datasets/enron/3_enron_by_day.txt\", \"datasets/enron/4_enron_by_day.txt\"]  #\n","print (graphFilePath + \"*.txt\")\n","files = glob.glob(graphFilePath + \"*.txt\")\n","files = glob.glob(\"/content/data/*.txt\")\n","#files = glob.glob(\"/content/AnomalyDetection/datasets/p2p-Gnutella/*.txt\")\n","\n","\n","#print(files)\n","sorted_files = sorted(files, key=natural_key)\n","\n","# Create directed graph in networkx\n","list_All_Feature_Set = []\n","\n","#Loop through each graph file\n","for file in sorted_files:\n","    f = open(file, \"r\")\n","    dataGraph = f.readlines()\n","\n","    #Create networkx graph object\n","    G = nx.DiGraph()\n","\n","    # Add edges to Graph object from input file\n","    for line in dataGraph:\n","        nodeA, nodeB = line.split(\" \")\n","        # print(nodeA)\n","        # print(nodeB)\n","        nodeA.rstrip('\\r\\n')\n","        nodeB.rstrip('\\r\\n')\n","        nodeB = nodeB[:-1]\n","        G.add_edge(str(nodeA), str(nodeB))\n","\n","    #Generate dictionary with pagerank (quality) values for each node\n","    weighted_Feature_Set = nx.pagerank(G)\n","    # print(\"pageranke\")\n","    # print(weighted_Feature_Set)\n","\n","    #Add edge \"quality\" from graph to weighted feature set\n","    for a, b in G.edges():\n","        weighted_Feature_Set[str(a) + \" \" + str(b)] = (1/len(G.edges(a))) * weighted_Feature_Set[a]\n","    #THIS NEEDS TO BE IMPROVED!!!\n","    # print(\"ksajfaskfj\")\n","    # print(len(weighted_Feature_Set)) \n","    # print(weighted_Feature_Set)\n","\n","    #Convert every key to a md5 hash\n","    weighted_Feature_Set1 = {}\n","    for key, value in weighted_Feature_Set.items():\n","        sha1_hash = hashlib.sha1(str(key).encode() + str(value).encode()).hexdigest()\n","        binary_hash = bin(int(sha1_hash, 16))[2:]\n","        #Trim to fixed size of 128\n","        binary_hash = (binary_hash[:128]) if len(binary_hash) > 128 else binary_hash\n","        #print(len(binary_hash))\n","        # print(\"jksdjfskdfjdksfj\")\n","        # print(str(binary_hash))\n","        \n","        #Add sha1 hash with corresponding value to feature set\n","        weighted_Feature_Set1[str(binary_hash)] = value\n","        # print(\"shfskfhksf\")\n","        # print(weighted_Feature_Set)\n","        # Remove original key/value from feature set\n","        #del weighted_Feature_Set[key]\n","\n","    #Add file\n","    # print(\"dsnkdjskdjskds\")\n","    # print(len(weighted_Feature_Set1))\n","    # print(weighted_Feature_Set1)\n","    \n","    list_All_Feature_Set.append(weighted_Feature_Set1)\n","\n","    #print (\"Processed file: \" + file)\n","\n","\n","#List of all document fingerprints\n","all_Fingerprints = []\n","\n","#Loop through each feature set to generate it's fingerprint\n","counter=0\n","for feature_Set in list_All_Feature_Set:\n","    temp_fingerprint = []\n","    #Build fingerprint\n","    #print(\"dnskjksfjsd\")\n","    # for key in feature_Set:\n","    #     print(key, ' : ', feature_Set[key])\n","    for key, val in feature_Set.items():\n","        binary_list = []\n","        #For each sha1 hash, if digit=1 then add weighted value. If digit=0, then add negative weighted value.\n","        #print(\"sjhfasjfhsaj\")\n","        #print(key, ' : ', val)\n","\n","        if( len(key) != 128):\n","          continue\n","\n","        for d in key:\n","            if int(d) == 1:\n","                binary_list.append(val)\n","            else:\n","                binary_list.append(-val)\n","        temp_fingerprint.append(binary_list)\n","  \n","    \n","     \n","\n","    #Calculate fingerprint by summing all the columns in temp_fingerprint\n","    fingerprint = [sum(x) for x in zip(*temp_fingerprint)]\n","  \n","\n","    #Convert all positive vals to '1' and all negative vals to '0' in fingerprint\n","    for index, item in enumerate(fingerprint):\n","        if item > 0:\n","            fingerprint[index] = 1\n","        else:\n","            fingerprint[index] = 0\n","    all_Fingerprints.append(fingerprint)\n","\n","f = numpy.array(all_Fingerprints)\n","#Calculate similarity scores (hamming distance) between each sequential pair of fingerprints\n","similarity_scores = []\n","\n","for x in range(len(all_Fingerprints) - 1):\n","    hamming = 0.0\n","    #Calculate hamming distance\n","    for index in range(0, 128):\n","        if all_Fingerprints[x][index] == all_Fingerprints[x + 1][index]:\n","            hamming += 1\n","    similarity_scores.append(1-(hamming/128))\n","    print (\"Similarity score between \" + str(x) + \" and \" + str(x+1) + \" : \" + str(1-(hamming / 128)))\n","print(\"jdjfhdkhfk\")\n","# print(counter)\n","# counter+=1\n","print(similarity_scores)\n","print(len(similarity_scores))\n","#Moving Range Average\n","median = numpy.median(numpy.array(similarity_scores))\n","# print(\"jdkjsfk\")\n","# print(median)\n","thresholdList = []\n","tempAvgNumerator = 0\n","multiplierForMR = 3\n","for y in range(len(similarity_scores)):\n","    if y == 0:\n","        tempAvgNumerator = 0\n","        thresholdList.append(0)\n","    else:\n","        tempAvgNumerator = tempAvgNumerator + abs(similarity_scores[y]-similarity_scores[y-1])\n","        threshold = median - (multiplierForMR*tempAvgNumerator/y)\n","        thresholdList.append(threshold)\n","\n","print(\"thrs\")\n","print(len(thresholdList))\n","print(thresholdList)\n","\n","#Detect anomalies || Look for two consecutive below threshold\n","anomalies = {}\n","anomaliesscore = {}\n","score = []\n","for x in range(len(similarity_scores)-1):\n","    #Detect two consecutive anomalies\n","    if (similarity_scores[x] < thresholdList[x]) and (similarity_scores[x+1] < thresholdList[x+1]):\n","        anomalies[str(x+1)] = abs(similarity_scores[x] - thresholdList[x]) + abs(similarity_scores[x+1] - thresholdList[x+1])\n","        anomaliesscore[str(x+1)] = similarity_scores[x+1]\n","\n","# print(\"anomaly\")\n","# print(anomalies)\n","# print(\"jhdshdh\")\n","# print(sorted(anomaliesscore.items(), key=operator.itemgetter(1), reverse=False))\n","\n","###Output Anomalies###\n","#You will list all of the anomalous time points if\n","#there are fewer than 10, the top 10 if there are fewer than 100, or the top 10% if there are\n","#more than 100.\n","f = open('anomalies_output', 'w')\n","sorted_anomalies = sorted(anomalies.items(), key=operator.itemgetter(1), reverse=True)\n","print(\"skdfhskdhfksd\")\n","print(sorted_anomalies)\n","numOfAnomalies = len(sorted_anomalies)\n","if numOfAnomalies > 100:\n","    print (\"There are \" + str(numOfAnomalies) + \" anomalies found, so we will output the top 10%\")\n","    for x in range(0,int(math.ceil(0.1*len(sorted_anomalies)))):\n","        f.write(str(sorted_anomalies[x][0]))\n","        f.write(\"\\n\")\n","elif numOfAnomalies > 11:\n","    print (\"There are \" + str(numOfAnomalies) + \" anomalies found, so we will output the top 10\")\n","    for x in range(0,10):\n","        f.write(str(sorted_anomalies[x][0]))\n","        # print(str(sorted_anomalies[x][0]))\n","        f.write(\"\\n\")\n","else:\n","    print (\"There are \" + str(numOfAnomalies) + \" anomalies found, so we will output all of them\")\n","    for x in sorted_anomalies:\n","        f.write(str(x[0]))\n","        f.write(\"\\n\")\n","f.close()\n","\n","print (\"Complete\")\n","print(\"--- Total processing took %s seconds ---\" % ( time.clock() - start_time))\n","\n","##DEBUGGING AND GRAPHING OUTPUT\n","print (\"Writing out to file: anomalies_output\")\n","#Write out to file\n","f = open('similarity_scores', 'w')\n","for x in similarity_scores:\n","    f.write(str(x))\n","    f.write(\"\\n\")\n","f.close()\n","\n","f = open('moving_thresholds', 'w')\n","for x in thresholdList:\n","    f.write(str(x))\n","    f.write(\"\\n\")\n","f.close()"],"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Loading files from folder: /content/data\n","/content/data*.txt\n","jdjfhdkhfk\n","[]\n","0\n","thrs\n","0\n","[]\n","skdfhskdhfksd\n","[]\n","There are 0 anomalies found, so we will output all of them\n","Complete\n","--- Total processing took 0.010510999999999715 seconds ---\n","Writing out to file: anomalies_output\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:22: DeprecationWarning: time.clock has been deprecated in Python 3.3 and will be removed from Python 3.8: use time.perf_counter or time.process_time instead\n","/usr/local/lib/python3.7/dist-packages/numpy/core/fromnumeric.py:3373: RuntimeWarning: Mean of empty slice.\n","  out=out, **kwargs)\n","/usr/local/lib/python3.7/dist-packages/numpy/core/_methods.py:170: RuntimeWarning: invalid value encountered in double_scalars\n","  ret = ret.dtype.type(ret / rcount)\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:220: DeprecationWarning: time.clock has been deprecated in Python 3.3 and will be removed from Python 3.8: use time.perf_counter or time.process_time instead\n"]}]}]}