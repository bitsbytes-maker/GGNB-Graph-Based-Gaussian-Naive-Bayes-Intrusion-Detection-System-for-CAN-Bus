{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"GNB_coding.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyN7b6RTqjAQ38u0xqqzneJi"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"F40VQh4GOMW8","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1620185450102,"user_tz":-360,"elapsed":3075,"user":{"displayName":"Maloy Kumar Devnath","photoUrl":"","userId":"08683219606020583205"}},"outputId":"064f040d-8e43-4184-d353-15f7623ff331"},"source":["\n","\n","import warnings\n","\n","from abc import ABCMeta, abstractmethod\n","\n","\n","import numpy as np\n","\n","import seaborn as sn\n","import matplotlib.pyplot as plt\n","import pandas\n","from sklearn.metrics import accuracy_score\n","from sklearn import naive_bayes as nb\n","import numpy as np\n","from sklearn.model_selection import train_test_split\n","\n","\n","from sklearn.base import BaseEstimator, ClassifierMixin\n","from sklearn.preprocessing import binarize\n","from sklearn.preprocessing import LabelBinarizer\n","from sklearn.preprocessing import label_binarize\n","from sklearn.utils import check_X_y, check_array, deprecated\n","from sklearn.utils.extmath import safe_sparse_dot\n","from sklearn.utils.fixes import logsumexp\n","from sklearn.utils.multiclass import _check_partial_fit_first_call\n","from sklearn.utils.validation import check_is_fitted, check_non_negative, column_or_1d\n","from sklearn.utils.validation import _check_sample_weight\n","\n","__all__ = ['BernoulliNB', 'GaussianNB', 'MultinomialNB', 'ComplementNB',\n","           'CategoricalNB']\n","\n","\n","class _BaseNB(ClassifierMixin, BaseEstimator, metaclass=ABCMeta):\n","    \"\"\"Abstract base class for naive Bayes estimators\"\"\"\n","\n","    @abstractmethod\n","    def _joint_log_likelihood(self, X):\n","        \"\"\"Compute the unnormalized posterior log probability of X\n","\n","        I.e. ``log P(c) + log P(x|c)`` for all rows x of X, as an array-like of\n","        shape [n_classes, n_samples].\n","\n","        Input is passed to _joint_log_likelihood as-is by predict,\n","        predict_proba and predict_log_proba.\n","        \"\"\"\n","\n","    def _check_X(self, X):\n","        \"\"\"To be overridden in subclasses with the actual checks.\"\"\"\n","        # Note that this is not marked @abstractmethod as long as the\n","        # deprecated public alias sklearn.naive_bayes.BayesNB exists\n","        # (until 0.24) to preserve backward compat for 3rd party projects\n","        # with existing derived classes.\n","        return X\n","\n","    def predict(self, X):\n","        \"\"\"\n","        Perform classification on an array of test vectors X.\n","\n","        Parameters\n","        ----------\n","        X : array-like of shape (n_samples, n_features)\n","\n","        Returns\n","        -------\n","        C : ndarray of shape (n_samples,)\n","            Predicted target values for X\n","        \"\"\"\n","        check_is_fitted(self)\n","        X = self._check_X(X)\n","        jll = self._joint_log_likelihood(X)\n","        return self.classes_[np.argmax(jll, axis=1)]\n","\n","    def predict_log_proba(self, X):\n","        \"\"\"\n","        Return log-probability estimates for the test vector X.\n","\n","        Parameters\n","        ----------\n","        X : array-like of shape (n_samples, n_features)\n","\n","        Returns\n","        -------\n","        C : array-like of shape (n_samples, n_classes)\n","            Returns the log-probability of the samples for each class in\n","            the model. The columns correspond to the classes in sorted\n","            order, as they appear in the attribute :term:`classes_`.\n","        \"\"\"\n","        check_is_fitted(self)\n","        X = self._check_X(X)\n","        jll = self._joint_log_likelihood(X)\n","        # normalize by P(x) = P(f_1, ..., f_n)\n","        log_prob_x = logsumexp(jll, axis=1)\n","        return jll - np.atleast_2d(log_prob_x).T\n","\n","    def predict_proba(self, X):\n","        \"\"\"\n","        Return probability estimates for the test vector X.\n","\n","        Parameters\n","        ----------\n","        X : array-like of shape (n_samples, n_features)\n","\n","        Returns\n","        -------\n","        C : array-like of shape (n_samples, n_classes)\n","            Returns the probability of the samples for each class in\n","            the model. The columns correspond to the classes in sorted\n","            order, as they appear in the attribute :term:`classes_`.\n","        \"\"\"\n","        return np.exp(self.predict_log_proba(X))\n","\n","\n","class GaussianNB(_BaseNB):\n","\n","\n","    def __init__(self, priors=None, var_smoothing=1e-9):\n","        self.priors = priors\n","        print(\"asjhask\")\n","        print(priors)\n","        self.var_smoothing = var_smoothing\n","        print(var_smoothing)\n","\n","    def fit(self, X, y, sample_weight=None):\n","\n","        y = column_or_1d(y, warn=True)\n","        print(\"--------y--------------\")\n","        print(y)\n","        return self._partial_fit(X, y, np.unique(y), _refit=True,\n","                                 sample_weight=sample_weight)\n","\n","    def _check_X(self, X):\n","        return check_array(X)\n","\n","    @staticmethod\n","    def _update_mean_variance(n_past, mu, var, X, sample_weight=None):\n","        \n","        if X.shape[0] == 0:\n","          return mu, var\n","\n","        # Compute (potentially weighted) mean and variance of new datapoints\n","        if sample_weight is not None:\n","            n_new = float(sample_weight.sum())\n","            new_mu = np.average(X, axis=0, weights=sample_weight)\n","            new_var = np.average((X - new_mu) ** 2, axis=0,\n","                                 weights=sample_weight)\n","        else:\n","            n_new = X.shape[0]\n","            new_var = np.var(X, axis=0)\n","            new_mu = np.mean(X, axis=0)\n","\n","            print(\"------------var+mu---------------\")\n","\n","            print(new_var)\n","            print(new_mu)\n","\n","\n","        if n_past == 0:\n","            print(\"wow\")\n","            return new_mu, new_var\n","\n","        n_total = float(n_past + n_new)\n","\n","        # Combine mean of old and new data, taking into consideration\n","        # (weighted) number of observations\n","        total_mu = (n_new * new_mu + n_past * mu) / n_total\n","        # Combine variance of old and new data, taking into consideration\n","        # (weighted) number of observations. This is achieved by combining\n","        # the sum-of-squared-differences (ssd)\n","        old_ssd = n_past * var\n","        new_ssd = n_new * new_var\n","        total_ssd = (old_ssd + new_ssd +\n","                     (n_new * n_past / n_total) * (mu - new_mu) ** 2)\n","        total_var = total_ssd / n_total\n","\n","        return total_mu, total_var\n","\n","    def partial_fit(self, X, y, classes=None, sample_weight=None):\n","       \n","        \n","        return self._partial_fit(X, y, classes, _refit=False,\n","                                 sample_weight=sample_weight)\n","\n","    def _partial_fit(self, X, y, classes=None, _refit=False,\n","                     sample_weight=None):\n","       \n","        print(\"Actual fitting GNB\")\n","        X, y = check_X_y(X, y)\n","        print(X)\n","        print(y)\n","        if sample_weight is not None:\n","            sample_weight = _check_sample_weight(sample_weight, X)\n","\n","        self.epsilon_ = self.var_smoothing * np.var(X, axis=0).max()\n","        print(\"Selfepsilom\")\n","        print( np.var(X, axis=0))\n","        print(self.epsilon_)\n","\n","        if _refit:\n","            self.classes_ = None\n","\n","        if _check_partial_fit_first_call(self, classes):\n","            # This is the first call to partial_fit:\n","            # initialize various cumulative counters\n","            n_features = X.shape[1]\n","            n_classes = len(self.classes_)\n","            self.theta_ = np.zeros((n_classes, n_features))\n","            self.sigma_ = np.zeros((n_classes, n_features))\n","            \n","\n","            self.class_count_ = np.zeros(n_classes, dtype=np.float64)\n","\n","            # Initialise the class prior\n","            # Take into account the priors\n","            if self.priors is not None:\n","                priors = np.asarray(self.priors)\n","                print(priors)\n","                # Check that the provide prior match the number of classes\n","                if len(priors) != n_classes:\n","                    raise ValueError('Number of priors must match number of'\n","                                     ' classes.')\n","                # Check that the sum is 1\n","                if not np.isclose(priors.sum(), 1.0):\n","                    raise ValueError('The sum of the priors should be 1.')\n","                # Check that the prior are non-negative\n","                if (priors < 0).any():\n","                    raise ValueError('Priors must be non-negative.')\n","                self.class_prior_ = priors\n","            else:\n","                # Initialize the priors to zeros for each class\n","                self.class_prior_ = np.zeros(len(self.classes_),\n","                                             dtype=np.float64)\n","        \n","        else:\n","            if X.shape[1] != self.theta_.shape[1]:\n","                msg = \"Number of features %d does not match previous data %d.\"\n","                raise ValueError(msg % (X.shape[1], self.theta_.shape[1]))\n","            # Put epsilon back in each time\n","            self.sigma_[:, :] -= self.epsilon_\n","\n","        classes = self.classes_\n","\n","        unique_y = np.unique(y)\n","        \n","        unique_y_in_classes = np.in1d(unique_y, classes)\n","        \n","\n","        if not np.all(unique_y_in_classes):\n","            raise ValueError(\"The target label(s) %s in y do not exist in the \"\n","                             \"initial classes %s\" %\n","                             (unique_y[~unique_y_in_classes], classes))\n","\n","        for y_i in unique_y:\n","            i = classes.searchsorted(y_i)\n","            X_i = X[y == y_i, :]\n","            print(str(y_i)+\" for the label\")\n","            print(X_i)\n","\n","            if sample_weight is not None:\n","                sw_i = sample_weight[y == y_i]\n","                N_i = sw_i.sum()\n","            else:\n","                sw_i = None\n","                N_i = X_i.shape[0]\n","                print(N_i)\n","\n","            new_theta, new_sigma = self._update_mean_variance(\n","                self.class_count_[i], self.theta_[i, :], self.sigma_[i, :],\n","                X_i, sw_i)\n","\n","            self.theta_[i, :] = new_theta\n","            self.sigma_[i, :] = new_sigma\n","            self.class_count_[i] += N_i\n","            print(self.class_count_[i])\n","            print(\"---------self theta-------------\")\n","            print(self.theta_)\n","            print(\"---------self sigma-------------\")\n","            print(self.sigma_)\n","\n","        self.sigma_[:, :] += self.epsilon_\n","\n","        # Update if only no priors is provided\n","        if self.priors is None:\n","            # Empirical prior, with sample_weight taken into account\n","            self.class_prior_ = self.class_count_ / self.class_count_.sum()\n","            print(self.class_count_)\n","            print(self.class_prior_)\n","            \n","\n","        return self\n","\n","    def _joint_log_likelihood(self, X):\n","        joint_log_likelihood = []\n","        for i in range(np.size(self.classes_)):\n","            jointi = np.log(self.class_prior_[i])\n","            print(\"Selffffffsigma\")\n","            \n","            print(self.sigma_[i,:])\n","            \n","            print(\"n_ij_first\")\n","            n_ij = - 0.5 * np.sum(np.log(2. * np.pi * self.sigma_[i, :]))\n","            print(n_ij)\n","            print(\"n_ij_final\")\n","            n_ij -= 0.5 * np.sum(((X - self.theta_[i, :]) ** 2) /\n","                                 (self.sigma_[i, :]), axis = 1)\n","            print(n_ij)\n","            \n","            joint_log_likelihood.append(jointi + n_ij)\n","\n","        joint_log_likelihood = np.array(joint_log_likelihood).T\n","        print(\"Joint_log_liklihood\")\n","        print(joint_log_likelihood)\n","        return joint_log_likelihood\n","\n","\n","\n","\n","\n","dataframe = pandas.read_table('/content/dos200_labels.txt')\n","data = dataframe.drop(\"No.\", axis=1)\n","\n","dataset = data.values\n","\n","\n","\n","# split into input (X) and output (Y) variables\n","training = 25825\n","testing = 36893 - 25825\n","\n","l = 36893\n","\n","X = dataset[:,0:]\n","# print(X.shape)\n","labels = []\n","for i in range (0,l):\n","  if X[i][9] == 1:\n","    labels.append(1)\n","  else:\n","    labels.append(0)\n","\n","features = X[0:l,0:9]\n","\n","\n","\n","\n","# Split our data\n","train, test, train_labels, test_labels = features[0:training],features[training:testing+training],labels[0:training],labels[training:testing+training]\n","\n","  # Initialize our classifier\n","\n","\n","\n","gnb = GaussianNB()\n","\n","\n","\n","\n","# Train our classifier\n","print(len(train_labels))\n","model =gnb.fit(train,train_labels)\n","preds = gnb.predict(test)\n","print(\"----------------------------GNB--------------------------\")\n","print(accuracy_score(test_labels, preds))\n"],"execution_count":3,"outputs":[{"output_type":"stream","text":["asjhask\n","None\n","1e-09\n","25825\n","--------y--------------\n","[1 1 1 ... 1 1 1]\n","Actual fitting GNB\n","[[2.40000000e+01 3.80000000e+01 4.00000000e+00 ... 4.18842832e-02\n","  8.42430489e-02 1.45552347e-02]\n"," [2.40000000e+01 3.70000000e+01 4.00000000e+00 ... 4.42486976e-02\n","  7.68504328e-02 1.52779076e-02]\n"," [2.50000000e+01 3.50000000e+01 3.00000000e+00 ... 4.22065228e-02\n","  8.61096838e-02 2.37975825e-02]\n"," ...\n"," [2.70000000e+01 4.10000000e+01 3.00000000e+00 ... 4.12082235e-02\n","  6.22576715e-02 1.67768202e-02]\n"," [2.40000000e+01 3.40000000e+01 3.00000000e+00 ... 4.19560708e-02\n","  8.43522175e-02 1.39475689e-02]\n"," [2.50000000e+01 4.90000000e+01 4.00000000e+00 ... 4.10179863e-02\n","  6.61039282e-02 1.52374725e-02]]\n","[1 1 1 ... 1 1 1]\n","Selfepsilom\n","[1.43927821e+00 7.44297638e+01 4.22075860e+01 2.32279032e-04\n"," 4.24778891e+01 3.05271632e-03 4.44918970e-05 1.32300762e-02\n"," 1.04442028e-05]\n","7.442976381538934e-08\n","0 for the label\n","[[2.50000000e+01 5.20000000e+01 2.50000000e+01 ... 2.14422587e-02\n","  4.54174424e-01 2.14422587e-02]\n"," [2.10000000e+01 4.70000000e+01 2.10000000e+01 ... 2.46673235e-02\n","  4.32948512e-01 2.46673235e-02]\n"," [2.20000000e+01 7.20000000e+01 2.00000000e+01 ... 2.77204564e-02\n","  3.09743407e-01 1.37515516e-02]\n"," ...\n"," [2.30000000e+01 6.10000000e+01 2.10000000e+01 ... 2.88308798e-02\n","  3.38726855e-01 1.90271308e-02]\n"," [2.10000000e+01 5.80000000e+01 2.10000000e+01 ... 3.02389031e-02\n","  3.58930577e-01 2.16713298e-02]\n"," [2.20000000e+01 4.90000000e+01 2.20000000e+01 ... 2.39370535e-02\n","  4.43065932e-01 2.39370535e-02]]\n","3992\n","------------var+mu---------------\n","[1.95306087e+00 9.43865074e+01 6.76015734e+00 1.50074698e-03\n"," 6.87575646e+00 1.24437251e-02 2.36253281e-05 5.58122241e-03\n"," 1.48665804e-05]\n","[2.27597695e+01 5.71791082e+01 2.16908818e+01 1.00150301e+00\n"," 2.17146794e+01 9.90981964e-01 2.65368808e-02 3.79460337e-01\n"," 1.99655490e-02]\n","wow\n","3992.0\n","---------self theta-------------\n","[[2.27597695e+01 5.71791082e+01 2.16908818e+01 1.00150301e+00\n","  2.17146794e+01 9.90981964e-01 2.65368808e-02 3.79460337e-01\n","  1.99655490e-02]\n"," [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n","  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n","  0.00000000e+00]]\n","---------self sigma-------------\n","[[1.95306087e+00 9.43865074e+01 6.76015734e+00 1.50074698e-03\n","  6.87575646e+00 1.24437251e-02 2.36253281e-05 5.58122241e-03\n","  1.48665804e-05]\n"," [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n","  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n","  0.00000000e+00]]\n","1 for the label\n","[[2.40000000e+01 3.80000000e+01 4.00000000e+00 ... 4.18842832e-02\n","  8.42430489e-02 1.45552347e-02]\n"," [2.40000000e+01 3.70000000e+01 4.00000000e+00 ... 4.42486976e-02\n","  7.68504328e-02 1.52779076e-02]\n"," [2.50000000e+01 3.50000000e+01 3.00000000e+00 ... 4.22065228e-02\n","  8.61096838e-02 2.37975825e-02]\n"," ...\n"," [2.70000000e+01 4.10000000e+01 3.00000000e+00 ... 4.12082235e-02\n","  6.22576715e-02 1.67768202e-02]\n"," [2.40000000e+01 3.40000000e+01 3.00000000e+00 ... 4.19560708e-02\n","  8.43522175e-02 1.39475689e-02]\n"," [2.50000000e+01 4.90000000e+01 4.00000000e+00 ... 4.10179863e-02\n","  6.61039282e-02 1.52374725e-02]]\n","21833\n","------------var+mu---------------\n","[7.11606556e-01 4.17653164e+01 8.76472508e-01 0.00000000e+00\n"," 7.13592857e-01 1.32650027e-03 1.00338706e-05 1.97047015e-04\n"," 6.30192615e-06]\n","[2.47845463e+01 4.34784959e+01 4.10374204e+00 1.00000000e+00\n"," 4.04287088e+00 9.98671735e-01 4.22721066e-02 7.39108149e-02\n"," 1.53216076e-02]\n","wow\n","21833.0\n","---------self theta-------------\n","[[2.27597695e+01 5.71791082e+01 2.16908818e+01 1.00150301e+00\n","  2.17146794e+01 9.90981964e-01 2.65368808e-02 3.79460337e-01\n","  1.99655490e-02]\n"," [2.47845463e+01 4.34784959e+01 4.10374204e+00 1.00000000e+00\n","  4.04287088e+00 9.98671735e-01 4.22721066e-02 7.39108149e-02\n","  1.53216076e-02]]\n","---------self sigma-------------\n","[[1.95306087e+00 9.43865074e+01 6.76015734e+00 1.50074698e-03\n","  6.87575646e+00 1.24437251e-02 2.36253281e-05 5.58122241e-03\n","  1.48665804e-05]\n"," [7.11606556e-01 4.17653164e+01 8.76472508e-01 0.00000000e+00\n","  7.13592857e-01 1.32650027e-03 1.00338706e-05 1.97047015e-04\n","  6.30192615e-06]]\n","[ 3992. 21833.]\n","[0.1545789 0.8454211]\n","Selffffffsigma\n","[1.95306094e+00 9.43865075e+01 6.76015741e+00 1.50082141e-03\n"," 6.87575653e+00 1.24437996e-02 2.36997578e-05 5.58129684e-03\n"," 1.49410102e-05]\n","n_ij_first\n","6.120659344451588\n","n_ij_final\n","[-92.90731818 -49.31182665 -55.23072257 ... -57.94177499 -60.160548\n"," -57.23665216]\n","Selffffffsigma\n","[7.11606630e-01 4.17653165e+01 8.76472583e-01 7.44297638e-08\n"," 7.13592932e-01 1.32657470e-03 1.01083004e-05 1.97121445e-04\n"," 6.37635591e-06]\n","n_ij_first\n","17.78594280917531\n","n_ij_final\n","[-362.15185592   11.73954894   17.41213689 ...   17.29542047   16.59722296\n","   16.28486076]\n","Joint_log_liklihood\n","[[ -94.77436884 -362.31977635]\n"," [ -51.17887731   11.57162851]\n"," [ -57.09777322   17.24421646]\n"," ...\n"," [ -59.80882565   17.12750004]\n"," [ -62.02759865   16.42930253]\n"," [ -59.10370282   16.11694034]]\n","----------------------------GNB--------------------------\n","0.9968377303939284\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"lZn5AK3ROONx"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":274},"id":"6Bi0FewD4RWw","executionInfo":{"status":"error","timestamp":1617244305728,"user_tz":-360,"elapsed":1289,"user":{"displayName":"Maloy Kumar Devnath","photoUrl":"","userId":"08683219606020583205"}},"outputId":"03d0595d-095d-40e7-b8e3-e5f42dd4d415"},"source":["import warnings\n","\n","from abc import ABCMeta, abstractmethod\n","\n","\n","import numpy as np\n","\n","import seaborn as sn\n","import matplotlib.pyplot as plt\n","import pandas\n","from sklearn.metrics import accuracy_score\n","from sklearn import naive_bayes as nb\n","import numpy as np\n","from sklearn.model_selection import train_test_split\n","\n","dataframe = pandas.read_table('/content/mixed200.txt')\n","data = dataframe.drop(\"No.\", axis=1)\n","data = data.drop(\"StdPageRank\", axis=1)\n","data = data.drop(\"VarPageRank\", axis=1)\n","dataset = data.values\n","\n","\n","\n","# split into input (X) and output (Y) variables\n","training = 57748\n","testing = 24749\n","\n","l = 82497\n","X = dataset[:,0:]\n","# print(X.shape)\n","labels = []\n","for i in range (0,l):\n","  if X[i][9] == 1:\n","    labels.append(1)\n","  else:\n","    labels.append(0)\n","\n","features = X[0:l,0:9]\n","\n","print(labels.count(0))\n","print(labels.count(1))\n","\n","\n","# Split our data\n","train, test, train_labels, test_labels = features[0:training],features[training:testing+training],labels[0:training],labels[training:testing+training]\n","\n","\n","\n","\n","gnb = svm.SVC()\n","\n","\n","\n","\n","# Train our classifier\n","print(len(train_labels))\n","model =gnb.fit(train,train_labels)\n","preds = gnb.predict(test)\n","print(\"----------------------------SVM--------------------------\")\n","print(accuracy_score(test_labels, preds))\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["23663\n","58834\n"],"name":"stdout"},{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-7-b3f847b936c3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m \u001b[0mgnb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSVC\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'svm' is not defined"]}]}]}