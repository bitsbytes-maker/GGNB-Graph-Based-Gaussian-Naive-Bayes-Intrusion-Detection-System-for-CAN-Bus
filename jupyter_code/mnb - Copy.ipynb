{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"mnb.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyMl5GCvsHEbXMgL95ywpdh8"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"Ir13-o1gi4H6","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1617100990491,"user_tz":-360,"elapsed":2622,"user":{"displayName":"Maloy Kumar Devnath","photoUrl":"","userId":"08683219606020583205"}},"outputId":"f942de17-c26f-4cea-d156-7e9bcc1fc999"},"source":["\n","import warnings\n","\n","from abc import ABCMeta, abstractmethod\n","\n","\n","import numpy as np\n","\n","from sklearn.base import BaseEstimator, ClassifierMixin\n","from sklearn.preprocessing import binarize\n","from sklearn.preprocessing import LabelBinarizer\n","from sklearn.preprocessing import label_binarize\n","from sklearn.utils import check_X_y, check_array, deprecated\n","from sklearn.utils.extmath import safe_sparse_dot\n","from sklearn.utils.fixes import logsumexp\n","from sklearn.utils.multiclass import _check_partial_fit_first_call\n","from sklearn.utils.validation import check_is_fitted, check_non_negative, column_or_1d\n","from sklearn.utils.validation import _check_sample_weight\n","\n","__all__ = ['BernoulliNB', 'GaussianNB', 'MultinomialNB', 'ComplementNB',\n","           'CategoricalNB']\n","\n","\n","class _BaseNB(ClassifierMixin, BaseEstimator, metaclass=ABCMeta):\n","    \"\"\"Abstract base class for naive Bayes estimators\"\"\"\n","\n","    @abstractmethod\n","    def _joint_log_likelihood(self, X):\n","        \"\"\"Compute the unnormalized posterior log probability of X\n","\n","        I.e. ``log P(c) + log P(x|c)`` for all rows x of X, as an array-like of\n","        shape [n_classes, n_samples].\n","\n","        Input is passed to _joint_log_likelihood as-is by predict,\n","        predict_proba and predict_log_proba.\n","        \"\"\"\n","\n","    def _check_X(self, X):\n","        \"\"\"To be overridden in subclasses with the actual checks.\"\"\"\n","        # Note that this is not marked @abstractmethod as long as the\n","        # deprecated public alias sklearn.naive_bayes.BayesNB exists\n","        # (until 0.24) to preserve backward compat for 3rd party projects\n","        # with existing derived classes.\n","        return X\n","\n","    def predict(self, X):\n","        \"\"\"\n","        Perform classification on an array of test vectors X.\n","\n","        Parameters\n","        ----------\n","        X : array-like of shape (n_samples, n_features)\n","\n","        Returns\n","        -------\n","        C : ndarray of shape (n_samples,)\n","            Predicted target values for X\n","        \"\"\"\n","        check_is_fitted(self)\n","        X = self._check_X(X)\n","        jll = self._joint_log_likelihood(X)\n","        return self.classes_[np.argmax(jll, axis=1)]\n","\n","    def predict_log_proba(self, X):\n","        \"\"\"\n","        Return log-probability estimates for the test vector X.\n","\n","        Parameters\n","        ----------\n","        X : array-like of shape (n_samples, n_features)\n","\n","        Returns\n","        -------\n","        C : array-like of shape (n_samples, n_classes)\n","            Returns the log-probability of the samples for each class in\n","            the model. The columns correspond to the classes in sorted\n","            order, as they appear in the attribute :term:`classes_`.\n","        \"\"\"\n","        check_is_fitted(self)\n","        X = self._check_X(X)\n","        jll = self._joint_log_likelihood(X)\n","        # normalize by P(x) = P(f_1, ..., f_n)\n","        log_prob_x = logsumexp(jll, axis=1)\n","        return jll - np.atleast_2d(log_prob_x).T\n","\n","    def predict_proba(self, X):\n","        \"\"\"\n","        Return probability estimates for the test vector X.\n","\n","        Parameters\n","        ----------\n","        X : array-like of shape (n_samples, n_features)\n","\n","        Returns\n","        -------\n","        C : array-like of shape (n_samples, n_classes)\n","            Returns the probability of the samples for each class in\n","            the model. The columns correspond to the classes in sorted\n","            order, as they appear in the attribute :term:`classes_`.\n","        \"\"\"\n","        return np.exp(self.predict_log_proba(X))\n","\n","\n","\n","\n","_ALPHA_MIN = 1e-10\n","\n","\n","class _BaseDiscreteNB(_BaseNB):\n","    def _check_X(self, X):\n","        return check_array(X, accept_sparse='csr')\n","\n","    def _check_X_y(self, X, y):\n","        return check_X_y(X, y, accept_sparse='csr')\n","\n","    def _update_class_log_prior(self, class_prior=None):\n","        n_classes = len(self.classes_)\n","        if class_prior is not None:\n","            if len(class_prior) != n_classes:\n","                raise ValueError(\"Number of priors must match number of\"\n","                                 \" classes.\")\n","            self.class_log_prior_ = np.log(class_prior)\n","        elif self.fit_prior:\n","            with warnings.catch_warnings():\n","                # silence the warning when count is 0 because class was not yet\n","                # observed\n","                warnings.simplefilter(\"ignore\", RuntimeWarning)\n","                log_class_count = np.log(self.class_count_)\n","\n","            # empirical prior, with sample_weight taken into account\n","            self.class_log_prior_ = (log_class_count -\n","                                     np.log(self.class_count_.sum()))\n","        else:\n","            self.class_log_prior_ = np.full(n_classes, -np.log(n_classes))\n","           \n","\n","\n","    def _check_alpha(self):\n","        if np.min(self.alpha) < 0:\n","            raise ValueError('Smoothing parameter alpha = %.1e. '\n","                             'alpha should be > 0.' % np.min(self.alpha))\n","        if isinstance(self.alpha, np.ndarray):\n","            if not self.alpha.shape[0] == self.n_features_:\n","                raise ValueError(\"alpha should be a scalar or a numpy array \"\n","                                 \"with shape [n_features]\")\n","        if np.min(self.alpha) < _ALPHA_MIN:\n","            warnings.warn('alpha too small will result in numeric errors, '\n","                          'setting alpha = %.1e' % _ALPHA_MIN)\n","            return np.maximum(self.alpha, _ALPHA_MIN)\n","        return self.alpha\n","\n","    def partial_fit(self, X, y, classes=None, sample_weight=None):\n","        X, y = self._check_X_y(X, y)\n","        _, n_features = X.shape\n","\n","        if _check_partial_fit_first_call(self, classes):\n","            # This is the first call to partial_fit:\n","            # initialize various cumulative counters\n","            n_effective_classes = len(classes) if len(classes) > 1 else 2\n","            self._init_counters(n_effective_classes, n_features)\n","            self.n_features_ = n_features\n","        elif n_features != self.n_features_:\n","            msg = \"Number of features %d does not match previous data %d.\"\n","            raise ValueError(msg % (n_features, self.n_features_))\n","\n","        Y = label_binarize(y, classes=self.classes_)\n","        if Y.shape[1] == 1:\n","            Y = np.concatenate((1 - Y, Y), axis=1)\n","\n","        if X.shape[0] != Y.shape[0]:\n","            msg = \"X.shape[0]=%d and y.shape[0]=%d are incompatible.\"\n","            raise ValueError(msg % (X.shape[0], y.shape[0]))\n","\n","        # label_binarize() returns arrays with dtype=np.int64.\n","        # We convert it to np.float64 to support sample_weight consistently\n","        Y = Y.astype(np.float64, copy=False)\n","        if sample_weight is not None:\n","            sample_weight = np.atleast_2d(sample_weight)\n","            Y *= check_array(sample_weight).T\n","\n","        class_prior = self.class_prior\n","\n","        # Count raw events from data before updating the class log prior\n","        # and feature log probas\n","        self._count(X, Y)\n","\n","        # XXX: OPTIM: we could introduce a public finalization method to\n","        # be called by the user explicitly just once after several consecutive\n","        # calls to partial_fit and prior any call to predict[_[log_]proba]\n","        # to avoid computing the smooth log probas at each call to partial fit\n","        alpha = self._check_alpha()\n","        self._update_feature_log_prob(alpha)\n","        self._update_class_log_prior(class_prior=class_prior)\n","        return self\n","\n","    def fit(self, X, y, sample_weight=None):\n","\n","        X, y = self._check_X_y(X, y)\n","        _, n_features = X.shape\n","        self.n_features_ = n_features\n","\n","        labelbin = LabelBinarizer()\n","        Y = labelbin.fit_transform(y)\n","        self.classes_ = labelbin.classes_\n","        if Y.shape[1] == 1:\n","            Y = np.concatenate((1 - Y, Y), axis=1)\n","\n","        # LabelBinarizer().fit_transform() returns arrays with dtype=np.int64.\n","        # We convert it to np.float64 to support sample_weight consistently;\n","        # this means we also don't have to cast X to floating point\n","        if sample_weight is not None:\n","            Y = Y.astype(np.float64, copy=False)\n","            sample_weight = np.asarray(sample_weight)\n","            sample_weight = np.atleast_2d(sample_weight)\n","            Y *= check_array(sample_weight).T\n","\n","        class_prior = self.class_prior\n","      \n","        # Count raw events from data before updating the class log prior\n","        # and feature log probas\n","        n_effective_classes = Y.shape[1]\n","\n","        self._init_counters(n_effective_classes, n_features)\n","        self._count(X, Y)\n","        alpha = self._check_alpha()\n","        self._update_feature_log_prob(alpha)\n","        self._update_class_log_prior(class_prior=class_prior)\n","        return self\n","\n","    def _init_counters(self, n_effective_classes, n_features):\n","        self.class_count_ = np.zeros(n_effective_classes, dtype=np.float64)\n","        self.feature_count_ = np.zeros((n_effective_classes, n_features),\n","                                       dtype=np.float64)\n","\n","    # XXX The following is a stopgap measure; we need to set the dimensions\n","    # of class_log_prior_ and feature_log_prob_ correctly.\n","    def _get_coef(self):\n","        return (self.feature_log_prob_[1:]\n","                if len(self.classes_) == 2 else self.feature_log_prob_)\n","\n","    def _get_intercept(self):\n","        return (self.class_log_prior_[1:]\n","                if len(self.classes_) == 2 else self.class_log_prior_)\n","\n","    coef_ = property(_get_coef)\n","    intercept_ = property(_get_intercept)\n","\n","    def _more_tags(self):\n","        return {'poor_score': True}\n","\n","\n","class MultinomialNB(_BaseDiscreteNB):\n","\n","\n","    def __init__(self, alpha=1.0, fit_prior=True, class_prior=None):\n","        self.alpha = alpha\n","        self.fit_prior = fit_prior\n","        self.class_prior = class_prior\n","\n","    def _more_tags(self):\n","        return {'requires_positive_X': True}\n","\n","    def _count(self, X, Y):\n","        \"\"\"Count and smooth feature occurrences.\"\"\"\n","        check_non_negative(X, \"MultinomialNB (input X)\")\n","        self.feature_count_ += safe_sparse_dot(Y.T, X)\n","        self.class_count_ += Y.sum(axis=0)\n","\n","        print(\"count\")\n","        print(self.feature_count_)\n","        #print(self.class_count_)\n","\n","    def _update_feature_log_prob(self, alpha):\n","        \"\"\"Apply smoothing to raw counts and recompute log probabilities\"\"\"\n","        smoothed_fc = self.feature_count_ + alpha\n","        print(\"smoothed fc\")\n","        print(smoothed_fc)\n","        smoothed_cc = smoothed_fc.sum(axis=1)\n","        print(\"smoothed_cc\")\n","        print(smoothed_cc)\n","        print(np.log(smoothed_cc.reshape(-1, 1)))\n","\n","        self.feature_log_prob_ = (np.log(smoothed_fc) -\n","                                  np.log(smoothed_cc.reshape(-1, 1)))\n","        print(\"feature_iiiiiiiiiilog_prob\")\n","        print(self.feature_log_prob_)\n","\n","    def _joint_log_likelihood(self, X):\n","        \"\"\"Calculate the posterior log probability of the samples X\"\"\"\n","        print(\"jlllllllllllllll\")\n","        print(self.class_log_prior_)\n","        print((safe_sparse_dot(X, self.feature_log_prob_.T) +\n","                self.class_log_prior_))\n","        return (safe_sparse_dot(X, self.feature_log_prob_.T) +\n","                self.class_log_prior_)\n","\n","\n","\n","import seaborn as sn\n","import matplotlib.pyplot as plt\n","import pandas\n","from sklearn.metrics import accuracy_score\n","from sklearn import naive_bayes as nb\n","import numpy as np\n","from sklearn.model_selection import train_test_split\n","\n","dataframe = pandas.read_table('/content/mixed200.txt')\n","data = dataframe.drop(\"No.\", axis=1)\n","data = data.drop(\"StdPageRank\", axis=1)\n","data = data.drop(\"VarPageRank\", axis=1)\n","dataset = data.values\n","\n","\n","\n","# split into input (X) and output (Y) variables\n","training = 57748\n","testing = 24749\n","\n","l = 82497\n","X = dataset[:,0:]\n","# print(X.shape)\n","labels = []\n","for i in range (0,l):\n","  if X[i][9] == 1:\n","    labels.append(1)\n","  else:\n","    labels.append(0)\n","\n","features = X[0:l,0:9]\n","\n","print(labels.count(0))\n","print(labels.count(1))\n","\n","\n","# Split our data\n","train, test, train_labels, test_labels = features[0:training],features[training:testing+training],labels[0:training],labels[training:testing+training]\n","\n","  # Initialize our classifier\n","\n","\n","\n","gnb = MultinomialNB()\n","\n","\n","\n","\n","# Train our classifier\n","print(len(train_labels))\n","model =gnb.fit(train,train_labels)\n","preds = gnb.predict(test)\n","print(\"----------------------------MNB--------------------------\")\n","print(accuracy_score(test_labels, preds))\n","\n"],"execution_count":3,"outputs":[{"output_type":"stream","text":["23663\n","58834\n","57748\n","count\n","[[7.20184000e+05 1.39657100e+06 2.40487000e+05 1.47190000e+04\n","  2.34285000e+05 1.47360000e+04 4.03109598e+02 3.55455770e+03\n","  2.21042790e+02]\n"," [1.03530300e+06 1.82424200e+06 1.71753000e+05 4.10530000e+04\n","  1.67663000e+05 4.10510000e+04 1.76403744e+03 2.66697401e+03\n","  6.30614075e+02]]\n","smoothed fc\n","[[7.20185000e+05 1.39657200e+06 2.40488000e+05 1.47200000e+04\n","  2.34286000e+05 1.47370000e+04 4.04109598e+02 3.55555770e+03\n","  2.22042790e+02]\n"," [1.03530400e+06 1.82424300e+06 1.71754000e+05 4.10540000e+04\n","  1.67664000e+05 4.10520000e+04 1.76503744e+03 2.66797401e+03\n","  6.31614075e+02]]\n","smoothed_cc\n","[2625169.71008624 3286135.62551802]\n","[[14.7806561 ]\n"," [15.00522285]]\n","feature_iiiiiiiiiilog_prob\n","[[-1.2933927  -0.63112488 -2.39023063 -5.18369371 -2.41635823 -5.18253949\n","  -8.77896998 -6.6043889  -9.37778599]\n"," [-1.15501719 -0.58854719 -2.95140435 -4.3825793  -2.97550559 -4.38262802\n","  -7.52929567 -7.11614819 -8.55694428]]\n","jlllllllllllllll\n","[-1.25030703 -0.3374563 ]\n","[[ -98.68792502  -95.55158502]\n"," [-174.46471066 -190.60659628]\n"," [ -89.34750311  -86.11465643]\n"," ...\n"," [ -83.3187738   -79.03918297]\n"," [ -84.54366156  -80.18341683]\n"," [ -94.90566575  -92.81885544]]\n","----------------------------MNB--------------------------\n","0.9073093862378278\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MDbtcxqzi6V9","executionInfo":{"status":"ok","timestamp":1617085570762,"user_tz":-360,"elapsed":1184,"user":{"displayName":"Maloy Kumar Devnath","photoUrl":"","userId":"08683219606020583205"}},"outputId":"2114d433-778e-466e-f623-dbc1520c150c"},"source":[""],"execution_count":null,"outputs":[{"output_type":"stream","text":["23663\n","58834\n","57748\n","count\n","[[7.20184000e+05 1.39657100e+06 2.40487000e+05 1.47190000e+04\n","  2.34285000e+05 1.47360000e+04 4.03109598e+02 3.55455770e+03\n","  2.21042790e+02]\n"," [1.03530300e+06 1.82424200e+06 1.71753000e+05 4.10530000e+04\n","  1.67663000e+05 4.10510000e+04 1.76403744e+03 2.66697401e+03\n","  6.30614075e+02]]\n","[16540. 41208.]\n","smoothed fc\n","[[7.20185000e+05 1.39657200e+06 2.40488000e+05 1.47200000e+04\n","  2.34286000e+05 1.47370000e+04 4.04109598e+02 3.55555770e+03\n","  2.22042790e+02]\n"," [1.03530400e+06 1.82424300e+06 1.71754000e+05 4.10540000e+04\n","  1.67664000e+05 4.10520000e+04 1.76503744e+03 2.66797401e+03\n","  6.31614075e+02]]\n","smoothed_cc\n","[2625169.71008624 3286135.62551802]\n","[[14.7806561 ]\n"," [15.00522285]]\n","feature_log_prob\n","[[-1.2933927  -0.63112488 -2.39023063 -5.18369371 -2.41635823 -5.18253949\n","  -8.77896998 -6.6043889  -9.37778599]\n"," [-1.15501719 -0.58854719 -2.95140435 -4.3825793  -2.97550559 -4.38262802\n","  -7.52929567 -7.11614819 -8.55694428]]\n","jlllllllllllllll\n","[[ -98.68792502  -95.55158502]\n"," [-174.46471066 -190.60659628]\n"," [ -89.34750311  -86.11465643]\n"," ...\n"," [ -83.3187738   -79.03918297]\n"," [ -84.54366156  -80.18341683]\n"," [ -94.90566575  -92.81885544]]\n","----------------------------MNB--------------------------\n","0.9073093862378278\n"],"name":"stdout"}]}]}