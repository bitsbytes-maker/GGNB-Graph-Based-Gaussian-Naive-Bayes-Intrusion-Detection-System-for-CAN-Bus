{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"experiment_cnb.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyOTzKGrDCRSWNxMrc9W9R5p"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"aijDSn9YXGIy","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1617114650134,"user_tz":-360,"elapsed":2864,"user":{"displayName":"Maloy Kumar Devnath","photoUrl":"","userId":"08683219606020583205"}},"outputId":"b5127eaf-d277-4b61-8a90-513ec7236680"},"source":["# -*- coding: utf-8 -*-\n","\n","\"\"\"\n","The :mod:`sklearn.naive_bayes` module implements Naive Bayes algorithms. These\n","are supervised learning methods based on applying Bayes' theorem with strong\n","(naive) feature independence assumptions.\n","\"\"\"\n","\n","# Author: Vincent Michel <vincent.michel@inria.fr>\n","#         Minor fixes by Fabian Pedregosa\n","#         Amit Aides <amitibo@tx.technion.ac.il>\n","#         Yehuda Finkelstein <yehudaf@tx.technion.ac.il>\n","#         Lars Buitinck\n","#         Jan Hendrik Metzen <jhm@informatik.uni-bremen.de>\n","#         (parts based on earlier work by Mathieu Blondel)\n","#\n","# License: BSD 3 clause\n","import warnings\n","\n","from abc import ABCMeta, abstractmethod\n","\n","import seaborn as sn\n","import matplotlib.pyplot as plt\n","import pandas\n","from sklearn.metrics import accuracy_score\n","from sklearn import naive_bayes as nb\n","import numpy as np\n","from sklearn.model_selection import train_test_split\n","\n","\n","import numpy as np\n","\n","from sklearn.base import BaseEstimator, ClassifierMixin\n","from sklearn.preprocessing import binarize\n","from sklearn.preprocessing import LabelBinarizer\n","from sklearn.preprocessing import label_binarize\n","from sklearn.utils import check_X_y, check_array, deprecated\n","from sklearn.utils.extmath import safe_sparse_dot\n","from sklearn.utils.fixes import logsumexp\n","from sklearn.utils.multiclass import _check_partial_fit_first_call\n","from sklearn.utils.validation import check_is_fitted, check_non_negative, column_or_1d\n","from sklearn.utils.validation import _check_sample_weight\n","\n","__all__ = ['BernoulliNB', 'GaussianNB', 'MultinomialNB', 'ComplementNB',\n","           'CategoricalNB']\n","\n","\n","class _BaseNB(ClassifierMixin, BaseEstimator, metaclass=ABCMeta):\n","    \"\"\"Abstract base class for naive Bayes estimators\"\"\"\n","\n","    @abstractmethod\n","    def _joint_log_likelihood(self, X):\n","        \"\"\"Compute the unnormalized posterior log probability of X\n","\n","        I.e. ``log P(c) + log P(x|c)`` for all rows x of X, as an array-like of\n","        shape [n_classes, n_samples].\n","\n","        Input is passed to _joint_log_likelihood as-is by predict,\n","        predict_proba and predict_log_proba.\n","        \"\"\"\n","\n","    def _check_X(self, X):\n","        \"\"\"To be overridden in subclasses with the actual checks.\"\"\"\n","        # Note that this is not marked @abstractmethod as long as the\n","        # deprecated public alias sklearn.naive_bayes.BayesNB exists\n","        # (until 0.24) to preserve backward compat for 3rd party projects\n","        # with existing derived classes.\n","        return X\n","\n","    def predict(self, X):\n","        \"\"\"\n","        Perform classification on an array of test vectors X.\n","\n","        Parameters\n","        ----------\n","        X : array-like of shape (n_samples, n_features)\n","\n","        Returns\n","        -------\n","        C : ndarray of shape (n_samples,)\n","            Predicted target values for X\n","        \"\"\"\n","        print(\"I am in the naive bayes predict\")\n","        check_is_fitted(self)\n","        X = self._check_X(X)\n","        jll = self._joint_log_likelihood(X)\n","        print('hmmmmmmmmmmmmmm')\n","        print(self.classes_[np.argmax(jll, axis=1)])\n","        return self.classes_[np.argmax(jll, axis=1)]\n","\n","    def predict_log_proba(self, X):\n","        check_is_fitted(self)\n","        X = self._check_X(X)\n","        jll = self._joint_log_likelihood(X)\n","        # normalize by P(x) = P(f_1, ..., f_n)\n","        log_prob_x = logsumexp(jll, axis=1)\n","        return jll - np.atleast_2d(log_prob_x).T\n","\n","    def predict_proba(self, X):\n","        \"\"\"\n","        Return probability estimates for the test vector X.\n","\n","        Parameters\n","        ----------\n","        X : array-like of shape (n_samples, n_features)\n","\n","        Returns\n","        -------\n","        C : array-like of shape (n_samples, n_classes)\n","            Returns the probability of the samples for each class in\n","            the model. The columns correspond to the classes in sorted\n","            order, as they appear in the attribute :term:`classes_`.\n","        \"\"\"\n","        return np.exp(self.predict_log_proba(X))\n","\n","\n","\n","\n","_ALPHA_MIN = 1e-10\n","\n","\n","class _BaseDiscreteNB(_BaseNB):\n","    \"\"\"Abstract base class for naive Bayes on discrete/categorical data\n","\n","    Any estimator based on this class should provide:\n","\n","    __init__\n","    _joint_log_likelihood(X) as per _BaseNB\n","    \"\"\"\n","\n","    def _check_X(self, X):\n","        return check_array(X, accept_sparse='csr')\n","\n","    def _check_X_y(self, X, y):\n","        return check_X_y(X, y, accept_sparse='csr')\n","\n","    def _update_class_log_prior(self, class_prior=None):\n","        n_classes = len(self.classes_)\n","        print(\"n_classes\")\n","        \n","        if class_prior is not None:\n","            if len(class_prior) != n_classes:\n","                raise ValueError(\"Number of priors must match number of\"\n","                                 \" classes.\")\n","            self.class_log_prior_ = np.log(class_prior)\n","            \n","            \n","        elif self.fit_prior:\n","            with warnings.catch_warnings():\n","                # silence the warning when count is 0 because class was not yet\n","                # observed\n","                warnings.simplefilter(\"ignore\", RuntimeWarning)\n","                log_class_count = np.log(self.class_count_)\n","\n","                print(self.class_count_)\n","\n","                print(log_class_count)\n","\n","                print(np.log(self.class_count_.sum()))\n","\n","\n","            # empirical prior, with sample_weight taken into account\n","            self.class_log_prior_ = (log_class_count -\n","                                     np.log(self.class_count_.sum()))\n","        else:\n","            self.class_log_prior_ = np.full(n_classes, -np.log(n_classes))\n","        print(self.class_log_prior_ )\n","\n","    def _check_alpha(self):\n","        if np.min(self.alpha) < 0:\n","            raise ValueError('Smoothing parameter alpha = %.1e. '\n","                             'alpha should be > 0.' % np.min(self.alpha))\n","        if isinstance(self.alpha, np.ndarray):\n","            if not self.alpha.shape[0] == self.n_features_:\n","                raise ValueError(\"alpha should be a scalar or a numpy array \"\n","                                 \"with shape [n_features]\")\n","        if np.min(self.alpha) < _ALPHA_MIN:\n","            warnings.warn('alpha too small will result in numeric errors, '\n","                          'setting alpha = %.1e' % _ALPHA_MIN)\n","            return np.maximum(self.alpha, _ALPHA_MIN)\n","        return self.alpha\n","\n","    def partial_fit(self, X, y, classes=None, sample_weight=None):\n","        \"\"\"Incremental fit on a batch of samples.\n","\n","        This method is expected to be called several times consecutively\n","        on different chunks of a dataset so as to implement out-of-core\n","        or online learning.\n","\n","        This is especially useful when the whole dataset is too big to fit in\n","        memory at once.\n","\n","        This method has some performance overhead hence it is better to call\n","        partial_fit on chunks of data that are as large as possible\n","        (as long as fitting in the memory budget) to hide the overhead.\n","\n","        Parameters\n","        ----------\n","        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n","            Training vectors, where n_samples is the number of samples and\n","            n_features is the number of features.\n","\n","        y : array-like of shape (n_samples,)\n","            Target values.\n","\n","        classes : array-like of shape (n_classes) (default=None)\n","            List of all the classes that can possibly appear in the y vector.\n","\n","            Must be provided at the first call to partial_fit, can be omitted\n","            in subsequent calls.\n","\n","        sample_weight : array-like of shape (n_samples,), default=None\n","            Weights applied to individual samples (1. for unweighted).\n","\n","        Returns\n","        -------\n","        self : object\n","        \"\"\"\n","        X, y = self._check_X_y(X, y)\n","        _, n_features = X.shape\n","\n","        if _check_partial_fit_first_call(self, classes):\n","            # This is the first call to partial_fit:\n","            # initialize various cumulative counters\n","            n_effective_classes = len(classes) if len(classes) > 1 else 2\n","            self._init_counters(n_effective_classes, n_features)\n","            self.n_features_ = n_features\n","        elif n_features != self.n_features_:\n","            msg = \"Number of features %d does not match previous data %d.\"\n","            raise ValueError(msg % (n_features, self.n_features_))\n","\n","        Y = label_binarize(y, classes=self.classes_)\n","        if Y.shape[1] == 1:\n","            Y = np.concatenate((1 - Y, Y), axis=1)\n","\n","        if X.shape[0] != Y.shape[0]:\n","            msg = \"X.shape[0]=%d and y.shape[0]=%d are incompatible.\"\n","            raise ValueError(msg % (X.shape[0], y.shape[0]))\n","\n","        # label_binarize() returns arrays with dtype=np.int64.\n","        # We convert it to np.float64 to support sample_weight consistently\n","        Y = Y.astype(np.float64, copy=False)\n","        if sample_weight is not None:\n","            sample_weight = np.atleast_2d(sample_weight)\n","            Y *= check_array(sample_weight).T\n","\n","        class_prior = self.class_prior\n","\n","        # Count raw events from data before updating the class log prior\n","        # and feature log probas\n","        self._count(X, Y)\n","\n","        # XXX: OPTIM: we could introduce a public finalization method to\n","        # be called by the user explicitly just once after several consecutive\n","        # calls to partial_fit and prior any call to predict[_[log_]proba]\n","        # to avoid computing the smooth log probas at each call to partial fit\n","        alpha = self._check_alpha()\n","        self._update_feature_log_prob(alpha)\n","        self._update_class_log_prior(class_prior=class_prior)\n","        return self\n","\n","    def fit(self, X, y, sample_weight=None):\n","        \"\"\"Fit Naive Bayes classifier according to X, y\n","\n","        Parameters\n","        ----------\n","        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n","            Training vectors, where n_samples is the number of samples and\n","            n_features is the number of features.\n","\n","        y : array-like of shape (n_samples,)\n","            Target values.\n","\n","        sample_weight : array-like of shape (n_samples,), default=None\n","            Weights applied to individual samples (1. for unweighted).\n","\n","        Returns\n","        -------\n","        self : object\n","        \"\"\"\n","        X, y = self._check_X_y(X, y)\n","        _, n_features = X.shape\n","        self.n_features_ = n_features\n","\n","        labelbin = LabelBinarizer()\n","        Y = labelbin.fit_transform(y)\n","        print(\"fit\")\n","        print(Y.shape)\n","        self.classes_ = labelbin.classes_\n","        if Y.shape[1] == 1:\n","            Y = np.concatenate((1 - Y, Y), axis=1)\n","            print(\"comcaascdk\")\n","            print(Y.T)\n","\n","\n","        # LabelBinarizer().fit_transform() returns arrays with dtype=np.int64.\n","        # We convert it to np.float64 to support sample_weight consistently;\n","        # this means we also don't have to cast X to floating point\n","        if sample_weight is not None:\n","            Y = Y.astype(np.float64, copy=False)\n","            sample_weight = np.asarray(sample_weight)\n","            sample_weight = np.atleast_2d(sample_weight)\n","            Y *= check_array(sample_weight).T\n","\n","        class_prior = self.class_prior\n","\n","        # Count raw events from data before updating the class log prior\n","        # and feature log probas\n","        n_effective_classes = Y.shape[1]\n","\n","        self._init_counters(n_effective_classes, n_features)\n","        self._count(X, Y)\n","        alpha = self._check_alpha()\n","        self._update_feature_log_prob(alpha)\n","        self._update_class_log_prior(class_prior=class_prior)\n","        return self\n","\n","    def _init_counters(self, n_effective_classes, n_features):\n","        self.class_count_ = np.zeros(n_effective_classes, dtype=np.float64)\n","        self.feature_count_ = np.zeros((n_effective_classes, n_features),\n","                                       dtype=np.float64)\n","\n","    # XXX The following is a stopgap measure; we need to set the dimensions\n","    # of class_log_prior_ and feature_log_prob_ correctly.\n","    def _get_coef(self):\n","        return (self.feature_log_prob_[1:]\n","                if len(self.classes_) == 2 else self.feature_log_prob_)\n","\n","    def _get_intercept(self):\n","        return (self.class_log_prior_[1:]\n","                if len(self.classes_) == 2 else self.class_log_prior_)\n","\n","    coef_ = property(_get_coef)\n","    intercept_ = property(_get_intercept)\n","\n","    def _more_tags(self):\n","        return {'poor_score': True}\n","\n","\n","\n","class ComplementNB(_BaseDiscreteNB):\n","    def __init__(self, alpha=1.0, fit_prior=True, class_prior=None,\n","                 norm=False):\n","        self.alpha = alpha\n","        self.fit_prior = fit_prior\n","        self.class_prior = class_prior\n","        self.norm = norm\n","\n","    def _more_tags(self):\n","        return {'requires_positive_X': True}\n","\n","    def _count(self, X, Y):\n","        \"\"\"Count feature occurrences.\"\"\"\n","        print(\"I am in the counter\")\n","        print(Y.T)\n","        print(Y.T.shape)\n","        check_non_negative(X, \"ComplementNB (input X)\")\n","        self.feature_count_ += safe_sparse_dot(Y.T, X)\n","        self.class_count_ += Y.sum(axis=0)\n","        self.feature_all_ = self.feature_count_.sum(axis=0)\n","        \n","        print(self.feature_count_)\n","        print(self.feature_all_)\n","\n","    def _update_feature_log_prob(self, alpha):\n","        \"\"\"Apply smoothing to raw counts and compute the weights.\"\"\"\n","        alpha = 1\n","        comp_count = self.feature_all_ + alpha - self.feature_count_\n","        print(\"-------------------comp_cpunt-------------\")\n","        print(comp_count)\n","        print(comp_count.sum(axis=1, keepdims=True))\n","        logged = np.log(comp_count / comp_count.sum(axis=1, keepdims=True))\n","        print(\"----------loggged------\")\n","        print(logged)\n","        # _BaseNB.predict uses argmax, but ComplementNB operates with argmin.\n","        if self.norm:\n","            summed = logged.sum(axis=1, keepdims=True)\n","            feature_log_prob = logged / summed\n","        else:\n","            feature_log_prob = -logged\n","            print(\"-------feature----------\")\n","            print(feature_log_prob)\n","        self.feature_log_prob_ = feature_log_prob\n","\n","    def _joint_log_likelihood(self, X):\n","        \"\"\"Calculate the class scores for the samples in X.\"\"\"\n","        print(\"x\")\n","        print(X)\n","        jll = safe_sparse_dot(X, self.feature_log_prob_.T)\n","        print(\"....jlllll........\")\n","        print(jll)\n","        if len(self.classes_) == 1:\n","            jll += self.class_log_prior_\n","            print(jll)\n","        return jll\n","\n","\n","\n","\n","\n","dataframe = pandas.read_table('/content/mixed200.txt')\n","data = dataframe.drop(\"No.\", axis=1)\n","data = data.drop(\"StdPageRank\", axis=1)\n","data = data.drop(\"VarPageRank\", axis=1)\n","dataset = data.values\n","\n","\n","\n","# split into input (X) and output (Y) variables\n","training = 57748\n","testing = 24749\n","\n","l = 82497\n","X = dataset[:,0:]\n","# print(X.shape)\n","labels = []\n","for i in range (0,l):\n","  if X[i][9] == 1:\n","    labels.append(1)\n","  else:\n","    labels.append(0)\n","\n","features = X[0:l,0:9]\n","\n","\n","\n","\n","# Split our data\n","train, test, train_labels, test_labels = features[0:training],features[training:testing+training],labels[0:training],labels[training:testing+training]\n","\n","  # Initialize our classifier\n","\n","\n","\n","gnb = ComplementNB()\n","\n","\n","\n","\n","# Train our classifier\n","print(len(train_labels))\n","model =gnb.fit(train,train_labels)\n","preds = gnb.predict(test)\n","print(\"----------------------------CNB--------------------------\")\n","print(accuracy_score(test_labels, preds))\n","\n","\n","\n","\n"],"execution_count":9,"outputs":[{"output_type":"stream","text":["57748\n","fit\n","(57748, 1)\n","comcaascdk\n","[[1 0 0 ... 1 1 0]\n"," [0 1 1 ... 0 0 1]]\n","I am in the counter\n","[[1 0 0 ... 1 1 0]\n"," [0 1 1 ... 0 0 1]]\n","(2, 57748)\n","[[7.20184000e+05 1.39657100e+06 2.40487000e+05 1.47190000e+04\n","  2.34285000e+05 1.47360000e+04 4.03109598e+02 3.55455770e+03\n","  2.21042790e+02]\n"," [1.03530300e+06 1.82424200e+06 1.71753000e+05 4.10530000e+04\n","  1.67663000e+05 4.10510000e+04 1.76403744e+03 2.66697401e+03\n","  6.30614075e+02]]\n","[1.75548700e+06 3.22081300e+06 4.12240000e+05 5.57720000e+04\n"," 4.01948000e+05 5.57870000e+04 2.16714703e+03 6.22153171e+03\n"," 8.51656865e+02]\n","-------------------comp_cpunt-------------\n","[[1.03530400e+06 1.82424300e+06 1.71754000e+05 4.10540000e+04\n","  1.67664000e+05 4.10520000e+04 1.76503744e+03 2.66797401e+03\n","  6.31614075e+02]\n"," [7.20185000e+05 1.39657200e+06 2.40488000e+05 1.47200000e+04\n","  2.34286000e+05 1.47370000e+04 4.04109598e+02 3.55555770e+03\n","  2.22042790e+02]]\n","[[3286135.62551802]\n"," [2625169.71008624]]\n","----------loggged------\n","[[-1.15501719 -0.58854719 -2.95140435 -4.3825793  -2.97550559 -4.38262802\n","  -7.52929567 -7.11614819 -8.55694428]\n"," [-1.2933927  -0.63112488 -2.39023063 -5.18369371 -2.41635823 -5.18253949\n","  -8.77896998 -6.6043889  -9.37778599]]\n","-------feature----------\n","[[1.15501719 0.58854719 2.95140435 4.3825793  2.97550559 4.38262802\n","  7.52929567 7.11614819 8.55694428]\n"," [1.2933927  0.63112488 2.39023063 5.18369371 2.41635823 5.18253949\n","  8.77896998 6.6043889  9.37778599]]\n","n_classes\n","[16540. 41208.]\n","[ 9.71353697 10.62638769]\n","10.963843995749706\n","[-1.25030703 -0.3374563 ]\n","I am in the naive bayes predict\n","x\n","[[2.50000000e+01 5.10000000e+01 4.00000000e+00 ... 4.32875295e-02\n","  6.25155845e-02 1.21123891e-02]\n"," [2.10000000e+01 5.00000000e+01 2.10000000e+01 ... 2.40334915e-02\n","  4.17289654e-01 2.40334915e-02]\n"," [2.50000000e+01 4.00000000e+01 4.00000000e+00 ... 4.51597518e-02\n","  5.95699347e-02 1.43867077e-02]\n"," ...\n"," [2.50000000e+01 3.80000000e+01 3.00000000e+00 ... 4.45465647e-02\n","  5.18965698e-02 2.46418316e-02]\n"," [2.50000000e+01 4.00000000e+01 3.00000000e+00 ... 4.62229954e-02\n","  5.37665200e-02 1.77714237e-02]\n"," [2.40000000e+01 4.30000000e+01 5.00000000e+00 ... 4.09192236e-02\n","  8.37027576e-02 1.75256946e-02]]\n","....jlllll........\n","[[ 95.21412872  97.43761799]\n"," [190.26913997 173.21440363]\n"," [ 85.77720013  88.09719608]\n"," ...\n"," [ 78.70172667  82.06846677]\n"," [ 79.84596053  83.29335454]\n"," [ 92.48139913  93.65535872]]\n","hmmmmmmmmmmmmmm\n","[1 0 1 ... 1 1 1]\n","----------------------------CNB--------------------------\n","0.9053295082629601\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"f8WhdBoQJ_gO","executionInfo":{"status":"ok","timestamp":1617110572657,"user_tz":-360,"elapsed":1602,"user":{"displayName":"Maloy Kumar Devnath","photoUrl":"","userId":"08683219606020583205"}},"outputId":"311b07e8-6057-4537-f323-c86ef9bde75d"},"source":[""],"execution_count":4,"outputs":[{"output_type":"stream","text":["57748\n","fit\n","(57748, 1)\n","comcaascdk\n","[[1 0 0 ... 1 1 0]\n"," [0 1 1 ... 0 0 1]]\n","I am in the counter\n","[[1 0 0 ... 1 1 0]\n"," [0 1 1 ... 0 0 1]]\n","(2, 57748)\n","[[7.20184000e+05 1.39657100e+06 2.40487000e+05 1.47190000e+04\n","  2.34285000e+05 1.47360000e+04 4.03109598e+02 3.55455770e+03\n","  2.21042790e+02]\n"," [1.03530300e+06 1.82424200e+06 1.71753000e+05 4.10530000e+04\n","  1.67663000e+05 4.10510000e+04 1.76403744e+03 2.66697401e+03\n","  6.30614075e+02]]\n","[1.75548700e+06 3.22081300e+06 4.12240000e+05 5.57720000e+04\n"," 4.01948000e+05 5.57870000e+04 2.16714703e+03 6.22153171e+03\n"," 8.51656865e+02]\n","-------------------comp_cpunt-------------\n","[[1.03530400e+06 1.82424300e+06 1.71754000e+05 4.10540000e+04\n","  1.67664000e+05 4.10520000e+04 1.76503744e+03 2.66797401e+03\n","  6.31614075e+02]\n"," [7.20185000e+05 1.39657200e+06 2.40488000e+05 1.47200000e+04\n","  2.34286000e+05 1.47370000e+04 4.04109598e+02 3.55555770e+03\n","  2.22042790e+02]]\n","[[3286135.62551802]\n"," [2625169.71008624]]\n","----------loggged------\n","[[-1.15501719 -0.58854719 -2.95140435 -4.3825793  -2.97550559 -4.38262802\n","  -7.52929567 -7.11614819 -8.55694428]\n"," [-1.2933927  -0.63112488 -2.39023063 -5.18369371 -2.41635823 -5.18253949\n","  -8.77896998 -6.6043889  -9.37778599]]\n","-------feature----------\n","[[1.15501719 0.58854719 2.95140435 4.3825793  2.97550559 4.38262802\n","  7.52929567 7.11614819 8.55694428]\n"," [1.2933927  0.63112488 2.39023063 5.18369371 2.41635823 5.18253949\n","  8.77896998 6.6043889  9.37778599]]\n","n_classes\n","[16540. 41208.]\n","[ 9.71353697 10.62638769]\n","10.963843995749706\n","[-1.25030703 -0.3374563 ]\n","I am in the naive bayes predict\n","x\n","[[2.50000000e+01 5.10000000e+01 4.00000000e+00 ... 4.32875295e-02\n","  6.25155845e-02 1.21123891e-02]\n"," [2.10000000e+01 5.00000000e+01 2.10000000e+01 ... 2.40334915e-02\n","  4.17289654e-01 2.40334915e-02]\n"," [2.50000000e+01 4.00000000e+01 4.00000000e+00 ... 4.51597518e-02\n","  5.95699347e-02 1.43867077e-02]\n"," ...\n"," [2.50000000e+01 3.80000000e+01 3.00000000e+00 ... 4.45465647e-02\n","  5.18965698e-02 2.46418316e-02]\n"," [2.50000000e+01 4.00000000e+01 3.00000000e+00 ... 4.62229954e-02\n","  5.37665200e-02 1.77714237e-02]\n"," [2.40000000e+01 4.30000000e+01 5.00000000e+00 ... 4.09192236e-02\n","  8.37027576e-02 1.75256946e-02]]\n","....jlllll........\n","[[ 95.21412872  97.43761799]\n"," [190.26913997 173.21440363]\n"," [ 85.77720013  88.09719608]\n"," ...\n"," [ 78.70172667  82.06846677]\n"," [ 79.84596053  83.29335454]\n"," [ 92.48139913  93.65535872]]\n","[1 0 1 ... 1 1 1]\n","----------------------------GNB--------------------------\n","0.9053295082629601\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"ywN_4h8BPkaX","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1617110657371,"user_tz":-360,"elapsed":965,"user":{"displayName":"Maloy Kumar Devnath","photoUrl":"","userId":"08683219606020583205"}},"outputId":"bdc971ee-c996-4e92-d38e-22e97695712b"},"source":["import seaborn as sn\n","import matplotlib.pyplot as plt\n","import pandas\n","from sklearn.metrics import accuracy_score\n","from sklearn import naive_bayes as nb\n","import numpy as np\n","from sklearn.model_selection import train_test_split\n","\n","\n","dataframe = pandas.read_table('/content/mixed200.txt')\n","data = dataframe.drop(\"No.\", axis=1)\n","data = data.drop(\"StdPageRank\", axis=1)\n","data = data.drop(\"VarPageRank\", axis=1)\n","dataset = data.values\n","\n","\n","\n","# split into input (X) and output (Y) variables\n","training = 57748\n","testing = 24749\n","\n","l = 82497\n","X = dataset[:,0:]\n","# print(X.shape)\n","labels = []\n","for i in range (0,l):\n","  if X[i][9] == 1:\n","    labels.append(1)\n","  else:\n","    labels.append(0)\n","\n","features = X[0:l,0:9]\n","\n","\n","\n","\n","# Split our data\n","train, test, train_labels, test_labels = features[0:training],features[training:testing+training],labels[0:training],labels[training:testing+training]\n","\n","  # Initialize our classifier\n","\n","\n","\n","gnb = nb.ComplementNB()\n","\n","\n","\n","\n","# Train our classifier\n","print(len(train_labels))\n","model =gnb.fit(train,train_labels)\n","preds = gnb.predict(test)\n","print(\"----------------------------CNB--------------------------\")\n","print(accuracy_score(test_labels, preds))\n","\n","\n"],"execution_count":5,"outputs":[{"output_type":"stream","text":["57748\n","----------------------------CNB--------------------------\n","0.9053295082629601\n"],"name":"stdout"}]}]}