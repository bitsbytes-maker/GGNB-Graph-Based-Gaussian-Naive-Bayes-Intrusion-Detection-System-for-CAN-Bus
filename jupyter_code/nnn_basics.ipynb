{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"nnn_basics.ipynb","provenance":[],"authorship_tag":"ABX9TyNuv2Fuv5hxmMfjwE338LZq"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":248},"id":"dHE-dmU3kei7","executionInfo":{"status":"error","timestamp":1622260526723,"user_tz":-360,"elapsed":448,"user":{"displayName":"Maloy Kumar Devnath","photoUrl":"","userId":"08683219606020583205"}},"outputId":"f6637a20-31b5-4c9e-d843-60b804f87bbc"},"source":["#from __future__ import print_function\n","import numpy as np \n","\n","\n","import matplotlib.pyplot as plt\n","\n","%matplotlib inline\n","np.random.seed(42)\n","\n","weightList=[]\n","biasList = []\n","\n","\n","\n","\n","\n","class ReLU():\n","    def __init__(self):\n","        # ReLU layer simply applies elementwise rectified linear unit to all inputs\n","        pass\n","    \n","    def forward(self, input):\n","        # Apply elementwise ReLU to [batch, input_units] matrix\n","        relu_forward = np.maximum(0,input)\n","        return relu_forward\n","    \n","    def forward1(self, input):\n","        # Apply elementwise ReLU to [batch, input_units] matrix\n","        relu_forward = np.maximum(0,input)\n","        return relu_forward\n","    \n","    \n","    def backward(self, input, grad_output):\n","        # Compute gradient of loss w.r.t. ReLU input\n","        relu_grad = input > 0\n","        #print(grad_output*relu_grad)\n","        return grad_output*relu_grad\n","\n","class Dense():\n","    def __init__(self, input_units, output_units, learning_rate=0.1):\n","        # A dense layer is a layer which performs a learned affine transformation:\n","        # f(x) = <W*x> + b\n","        \n","        self.learning_rate = learning_rate\n","        #self.weights = np.random.normal(loc=0.0, scale = np.sqrt(2/(input_units+output_units)), size = (input_units,output_units))\n","        #print(self.weights.shape)\n","        mu, sigma = 0, 0.1\n","        self.weights = np.random.normal(mu,sigma,size=(input_units, output_units))\n","        \n","        \n","        self.biases = np.zeros(output_units)\n","        \n","        \n","    def forward(self,input):\n","\n","        # print(input.shape)\n","        # print(self.weights.shape)\n","        # print(self.biases.shape)\n","        f = np.dot(input,self.weights) + self.biases\n","    \n","\n","        return f\n","    \n","    def forward1(self,input):\n","\n","        # print(input.shape)\n","        # print(self.weights.shape)\n","        # print(self.biases.shape)\n","        \n","        weightList.append(self.weights)\n","        biasList.append(self.biases)\n","        f = np.dot(input,self.weights) + self.biases\n","        #print(f.shape)\n","\n","        return f\n","    \n","    def backward(self,input,grad_output):\n","        # compute d f / d x = d f / d dense * d dense / d x\n","        # where d dense/ d x = weights transposed\n","        grad_input = np.dot(grad_output, self.weights.T)\n","        \n","        # compute gradient w.r.t. weights and biases\n","        grad_weights = np.dot(input.T, grad_output)\n","        grad_biases = grad_output.mean(axis=0)*input.shape[0]\n","        \n","        assert grad_weights.shape == self.weights.shape and grad_biases.shape == self.biases.shape\n","        \n","        # Here we perform a stochastic gradient descent step. \n","        self.weights = self.weights - self.learning_rate * grad_weights\n","        self.biases = self.biases - self.learning_rate * grad_biases\n","        \n","        return grad_input\n","\n","\n","def softmax_crossentropy_with_logits(logits,reference_answers):\n","    # Compute crossentropy from logits[batch,n_classes] and ids of correct answers\n","    logits_for_answers = logits[np.arange(len(logits)),reference_answers]\n","   \n","    part = np.log(np.sum(np.exp(logits),axis=-1))\n","\n","    \n","    xentropy = - logits_for_answers + part\n","    \n","    return xentropy\n","def grad_softmax_crossentropy_with_logits(logits,reference_answers):\n","    # Compute crossentropy gradient from logits[batch,n_classes] and ids of correct answers\n","    ones_for_answers = np.zeros_like(logits)\n","   \n","    ones_for_answers[np.arange(len(logits)),reference_answers] = 1\n","\n","   \n","    softmax = np.exp(logits) / np.exp(logits).sum(axis=-1,keepdims=True)\n","    \n","    return (- ones_for_answers + softmax) / logits.shape[0]\n","##2000, 1500, 1000, 500, 10 \n","\n","network = []\n","#print(X_train.shape[1])\n","network.append(Dense(9,50))\n","network.append(ReLU())\n","network.append(Dense(50,50))\n","network.append(ReLU())\n","network.append(Dense(50,50))\n","network.append(ReLU())\n","network.append(Dense(50,50))\n","network.append(ReLU())\n","network.append(Dense(50,2))\n","\n","def forward(network, X, k = 0):\n","    # Compute activations of all network layers by applying them sequentially.\n","    # Return a list of activations for each layer. \n","    \n","    activations = []\n","    input = X\n","    # Looping through each layer\n","    for l in network:\n","      if k == 0:\n","        activations.append(l.forward(input))\n","        input = activations[-1]\n","      else:\n","        activations.append(l.forward1(input))\n","        input = activations[-1]\n","\n","    return activations\n","def predict(network,X,k=0):\n","    # Compute network predictions. Returning indices of largest Logit probability\n","    logits = forward(network,X,k)[-1]\n","  \n","\n","\n","    return logits.argmax(axis=-1)\n","def train(network,X,y):\n","\n","    layer_activations = forward(network,X)\n","   \n","    # print(layer_activations[6].shape)\n","    \n","    layer_inputs = [X]+layer_activations  #layer_input[i] is an input for network[i]\n","\n","    #print(X.shape)\n","    \n","    logits = layer_activations[-1]\n","\n","    \n","    # Compute the loss and the initial gradient\n","    loss = softmax_crossentropy_with_logits(logits,y)\n","    loss_grad = grad_softmax_crossentropy_with_logits(logits,y)\n","   \n","    \n","    # Propagate gradients through the network\n","    # Reverse propogation as this is backprop\n","    for layer_index in range(len(network))[::-1]:\n","       \n","        layer = network[layer_index]\n","        \n","        loss_grad = layer.backward(layer_inputs[layer_index],loss_grad) #grad w.r.t. input, also weight updates\n","        \n","    return np.mean(loss)\n","\n","\n","\n","\n","\n","print(\"\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\")\n","\n","\n","\n","from tqdm import trange\n","def iterate_minibatches(inputs, targets, batchsize, shuffle=False):\n","    #assert len(inputs) == len(targets)\n","    if shuffle:\n","        indices = np.random.permutation(len(inputs))\n","    for start_idx in trange(0, len(inputs) - batchsize + 1, batchsize):\n","        if shuffle:\n","            excerpt = indices[start_idx:start_idx + batchsize]\n","        #print(len(targets[excerpt]))\n","        yield inputs[excerpt], targets[excerpt]\n","\n","train_log = []\n","val_log = []\n","for epoch in range(5):\n","  for x_batch,y_batch in iterate_minibatches(X_train,y_train,batchsize=32,shuffle=True):\n","    train(network,x_batch,y_batch)\n","\n","    \n","train_log.append(np.mean(predict(network,X_train)==y_train))\n","val_log.append(np.mean(predict(network,X_val)==y_val))\n","    \n","    #clear_output()\n","print(\"Epoch\",epoch)\n","print(\"Train accuracy:\",train_log[-1])\n","print(\"Val accuracy:\",val_log[-1])\n","\n","print(100*np.sum(predict(network,X_test,1)==y_test)/len(y_test))\n","\n"],"execution_count":1,"outputs":[{"output_type":"stream","text":["\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n"],"name":"stdout"},{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-f11ef806b1b0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    200\u001b[0m \u001b[0mval_log\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 202\u001b[0;31m   \u001b[0;32mfor\u001b[0m \u001b[0mx_batch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_batch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterate_minibatches\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatchsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    203\u001b[0m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnetwork\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx_batch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'X_train' is not defined"]}]}]}